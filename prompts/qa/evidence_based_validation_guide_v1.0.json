{
    "name": "evidence_based_validation_guide",
    "version": "1.0",
    "created": "2026-01-28",
    "purpose": "Guide AI and human QA engineers on when and how to use evidence-based validation vs structured testing",
    "context": "Derived from AAA v0.9 and v1.0 test coverage strategy, where observability and governance features used evidence-based validation successfully",
    "guidance": {
        "when_to_use_evidence_based_validation": {
            "appropriate_scenarios": [
                "Observability features (dashboards, metrics, reports) - outputs are artifacts themselves",
                "Policy enforcement mechanisms (rulesets, gates, workflows) - validation happens in production",
                "Meta-systems (self-dogfooding scenarios) - the system validates itself",
                "One-time migrations or bootstrap operations - repeatability not critical",
                "Features where execution = proof (e.g., report generation)"
            ],
            "inappropriate_scenarios": [
                "Core business logic requiring deterministic behavior",
                "Security-critical functions (must have structured tests)",
                "Parsing or data transformation (needs unit test coverage)",
                "API contracts (requires integration tests)",
                "User-facing features with complex state management"
            ]
        },
        "minimum_evidence_requirements": {
            "production_artifacts": {
                "count": "≥3 independent artifacts with timestamps",
                "examples": [
                    "Generated reports (JSON, MD, HTML)",
                    "Dashboard outputs",
                    "Audit logs",
                    "Workflow run results"
                ],
                "verification": "Artifacts must be dated and traceable to specific executions"
            },
            "traceability": {
                "required_elements": [
                    "Execution logs or command outputs",
                    "Commit SHAs linking to source code version",
                    "Timestamps proving independent runs",
                    "Evidence from ≥2 different execution contexts (e.g., local + CI)"
                ]
            },
            "repeatability": {
                "standard": "Evidence must come from ≥2 independent runs",
                "rationale": "Proves feature is not a one-time fluke"
            }
        },
        "adjusted_121_rule": {
            "formula": "0 unit + N evidence artifacts + 1 live workflow = PASS",
            "explanation": "For evidence-based features, artifact existence replaces traditional unit/integration tests",
            "minimum_thresholds": {
                "evidence_artifacts": "≥4 (e.g., JSON output, MD report, HTML dashboard, execution log)",
                "live_workflow": "≥1 (e.g., nightly automation, CI run, production deployment)"
            },
            "justification_required": true,
            "justification_template": {
                "test_strategy": "Evidence-Based Validation",
                "rationale": "[Why traditional testing is less effective than evidence]",
                "validation_approach": "[How evidence proves correctness]",
                "evidence_chain": "[Step-by-step evidence trail]",
                "compliance_interpretation": "[Adjusted 1+2+1 formula and reasoning]"
            }
        },
        "documentation_requirements": {
            "test_coverage_appendix": {
                "location": "Add to completion report",
                "required_sections": [
                    "Coverage Analysis (strategy + rationale)",
                    "Validation Approach (table showing test types)",
                    "Evidence Chain (step-by-step proof)",
                    "Compliance with 1+2+1 Rule (strict vs adjusted)",
                    "Future Improvements (if strict compliance needed)"
                ]
            },
            "evidence_linking": {
                "guideline": "All evidence artifacts must be linked with file paths or URLs",
                "examples": [
                    "Nightly report: `reports/audits/nightly_governance_YYYYMMDD_HHMM.md`",
                    "Dashboard: `docs/dashboard/index.html`",
                    "Workflow run: `https://github.com/org/repo/actions/runs/12345`"
                ]
            }
        }
    },
    "real_world_examples": {
        "v0.9_observability": {
            "feature": "Compliance dashboard MVP",
            "test_strategy": "Evidence-based",
            "evidence_artifacts": [
                "nightly_governance_20260123_1414.md",
                "github_audit_report_20260123_1414.md",
                "docs/dashboard/index.html",
                "aaa_v0.9_gate_evidence_20260123.md"
            ],
            "live_workflow": "nightly-governance.yaml",
            "adjusted_121": "0+4 evidence artifacts+1 live workflow = PASS",
            "justification": "Dashboard is output-centric; artifact existence = test passing"
        },
        "v1.0_governance": {
            "feature": "Enterprise gate enforcement",
            "test_strategy": "Evidence-based + Self-dogfooding",
            "evidence_artifacts": [
                "docs/manuals/admin/setup-ruleset.md",
                "aaa-actions/.github/workflows/reusable-gate.yaml",
                ".github/workflows/aaa-gate.yaml",
                "runner/checks/check_release_integrity.py"
            ],
            "live_workflow": "AAA org using governance-gate ruleset",
            "self_dogfooding_proof": "If broken, AAA's own development would fail",
            "adjusted_121": "0+4 evidence artifacts+1 self-dogfooding E2E = PASS",
            "justification": "Meta-system validation; operational success = test passing"
        }
    },
    "qa_engineer_checklist": {
        "before_choosing_evidence_based": [
            "✓ Is this feature primarily output-generating?",
            "✓ Would traditional unit tests provide limited value?",
            "✓ Can correctness be verified through artifacts?",
            "✓ Are there ≥3 different artifact types?",
            "✓ Is there a live workflow proving repeatability?"
        ],
        "during_validation": [
            "✓ Collect ≥4 evidence artifacts with timestamps",
            "✓ Ensure artifacts from ≥2 independent runs",
            "✓ Document complete evidence chain",
            "✓ Link all artifacts in completion report",
            "✓ Write test coverage appendix with justification"
        ],
        "after_validation": [
            "✓ Add Test Coverage Appendix to completion report",
            "✓ Update roadmap with validation status",
            "✓ Consider future structured test improvements (optional)"
        ]
    },
    "anti_patterns": {
        "avoid": [
            {
                "pattern": "Using evidence-based validation for security features",
                "why": "Security requires deterministic, reproducible tests"
            },
            {
                "pattern": "Only 1-2 evidence artifacts",
                "why": "Insufficient proof of robustness"
            },
            {
                "pattern": "Evidence without timestamps or traceability",
                "why": "Cannot verify independence or repeatability"
            },
            {
                "pattern": "No justification in completion report",
                "why": "Future reviewers won't understand the strategy"
            },
            {
                "pattern": "Using evidence-based as excuse for no testing",
                "why": "Must still prove correctness, just via different means"
            }
        ]
    },
    "recommendation": "Evidence-based validation is a legitimate QA strategy when used appropriately. It is not a compromise but a design choice for specific feature types. Always document the rationale clearly."
}